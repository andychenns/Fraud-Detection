{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD95 = True\n",
    "BUILD96 = True\n",
    "\n",
    "import numpy as np, pandas as pd, os, gc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# COLUMNS WITH STRINGS\n",
    "str_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n",
    "            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n",
    "            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n",
    "str_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n",
    "            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n",
    "\n",
    "# FIRST 53 COLUMNS\n",
    "cols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n",
    "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
    "       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n",
    "       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n",
    "       'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n",
    "# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\n",
    "v =  [1, 3, 4, 6, 8, 11]\n",
    "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
    "v += [36, 37, 40, 41, 44, 47, 48]\n",
    "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "\n",
    "#v += [96, 98, 99, 104] #relates to groups, no NAN \n",
    "v += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\n",
    "v += [124, 127, 129, 130, 136] # relates to groups, no NAN\n",
    "\n",
    "# LOTS OF NAN BELOW\n",
    "v += [138, 139, 142, 147, 156, 162] #b1\n",
    "v += [165, 160, 166] #b1\n",
    "v += [178, 176, 173, 182] #b2\n",
    "v += [187, 203, 205, 207, 215] #b2\n",
    "v += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\n",
    "v += [218, 223, 224, 226, 228, 229, 235] #b3\n",
    "v += [240, 258, 257, 253, 252, 260, 261] #b3\n",
    "v += [264, 266, 267, 274, 277] #b3\n",
    "v += [220, 221, 234, 238, 250, 271] #b3\n",
    "\n",
    "v += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\n",
    "v += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\n",
    "v += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n",
    "#v += [332, 325, 335, 338] # b4 lots NAN\n",
    "\n",
    "cols += ['V'+str(x) for x in v]\n",
    "dtypes = {}\n",
    "for c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n",
    "    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n",
    "        dtypes[c] = 'float32'\n",
    "for c in str_type: dtypes[c] = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (590540, 214) test shape (506691, 213)\n",
      "CPU times: user 19.2 s, sys: 1.05 s, total: 20.2 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD TRAIN\n",
    "X_train = pd.read_csv('../Data/train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\n",
    "train_id = pd.read_csv('../Data/train_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
    "X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n",
    "# LOAD TEST\n",
    "X_test = pd.read_csv('../Data/test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\n",
    "test_id = pd.read_csv('../Data/test_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
    "fix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\n",
    "test_id.rename(columns=fix, inplace=True)\n",
    "X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n",
    "# TARGET\n",
    "y_train = X_train['isFraud'].copy()\n",
    "del train_id, test_id; x = gc.collect()\n",
    "# PRINT STATUS\n",
    "print('Train shape',X_train.shape,'test shape',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE D COLUMNS\n",
    "for i in range(1,16):\n",
    "    if i in [1,2,3,5,9]: continue\n",
    "    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n",
    "    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'isFraud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'isFraud'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'isFraud'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LABEL ENCODE AND MEMORY REDUCE\n",
    "for i,f in enumerate(X_train.columns):\n",
    "    # FACTORIZE CATEGORICAL VARIABLES\n",
    "    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n",
    "        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n",
    "        df_comb,_ = df_comb.factorize(sort=True)\n",
    "        if df_comb.max()>32000: print(f,'needs int32')\n",
    "        X_train[f] = df_comb[:len(X_train)].astype('int16')\n",
    "        X_test[f] = df_comb[len(X_train):].astype('int16')\n",
    "    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n",
    "    elif f not in ['TransactionAmt','TransactionDT']:\n",
    "        mn = np.min((X_train[f].min(),X_test[f].min()))\n",
    "        X_train[f] -= np.float32(mn)\n",
    "        X_test[f] -= np.float32(mn)\n",
    "        X_train[f].fillna(-1,inplace=True)\n",
    "        X_test[f].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(nm,', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=X_train,test=X_test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(nm,', ',end='')\n",
    "        \n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "# COMBINE FEATURES by underscore\n",
    "def encode_CB(col1,col2,df1=X_train,df2=X_test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "    \n",
    "# GROUP AGGREGATION NUNIQUE count how many times a category repeats in a column\n",
    "def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'id': ['egg', 'egg', 'egg', 'egg',\n",
    "                          'ham', 'ham','ham'],\n",
    "                   'value1': [1, 0, 1, 1, 0, 0,1]})\n",
    "\n",
    "df.groupby('id')['value1'].agg(['mean']).to_dict()\n",
    "\n",
    "dict1 = {'egg': 0.75, 'ham': 0.3333333333333333}\n",
    "s = pd.Series(['egg', 'ham',\"egg\"])\n",
    "type(s.map(dict1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below creates new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cents, addr1_FE , card1_FE , card2_FE , card3_FE , P_emaildomain_FE , R_emaildomain_FE , card1_addr1 , card1_addr1_P_emaildomain , card1_addr1_FE , card1_addr1_P_emaildomain_FE , 'TransactionAmt_card1_mean' , 'TransactionAmt_card1_std' , 'TransactionAmt_card1_addr1_mean' , 'TransactionAmt_card1_addr1_std' , 'TransactionAmt_card1_addr1_P_emaildomain_mean' , 'TransactionAmt_card1_addr1_P_emaildomain_std' , 'D9_card1_mean' , 'D9_card1_std' , 'D9_card1_addr1_mean' , 'D9_card1_addr1_std' , 'D9_card1_addr1_P_emaildomain_mean' , 'D9_card1_addr1_P_emaildomain_std' , 'D11_card1_mean' , 'D11_card1_std' , 'D11_card1_addr1_mean' , 'D11_card1_addr1_std' , 'D11_card1_addr1_P_emaildomain_mean' , 'D11_card1_addr1_P_emaildomain_std' , CPU times: user 14.4 s, sys: 4.39 s, total: 18.8 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRANSACTION AMT CENTS\n",
    "X_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\n",
    "X_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\n",
    "print('cents, ', end='')\n",
    "# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n",
    "encode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain','R_emaildomain'])\n",
    "# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\n",
    "encode_CB('card1','addr1')\n",
    "encode_CB('card1_addr1','P_emaildomain')\n",
    "# FREQUENCY ENOCDE for new features\n",
    "encode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n",
    "# GROUP AGGREGATE\n",
    "encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to feature the time of transaction because the fraud follows a pattern in a day: higher the morning and lower in the afternoon/evening. We will need to calculate the hour of a transaction for both training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "X_train[\"Date\"] = X_train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "\n",
    "X_train['Weekdays'] = X_train['Date'].dt.dayofweek\n",
    "X_train['Hours'] = X_train['Date'].dt.hour\n",
    "X_train['Days'] = X_train['Date'].dt.day\n",
    "X_train['Month'] = X_train['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"Date\"] = X_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "\n",
    "X_test['Weekdays'] = X_test['Date'].dt.dayofweek\n",
    "X_test['Hours'] = X_test['Date'].dt.hour\n",
    "X_test['Days'] = X_test['Date'].dt.day\n",
    "X_test['Month'] = X_test['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID\n",
       "2987000.0     0\n",
       "2987001.0     0\n",
       "2987002.0     0\n",
       "2987003.0     0\n",
       "2987004.0     0\n",
       "             ..\n",
       "3577535.0    23\n",
       "3577536.0    23\n",
       "3577537.0    23\n",
       "3577538.0    23\n",
       "3577539.0    23\n",
       "Name: Hours, Length: 590540, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use several different approaches for feature selection. The first one is time consistancy, which check whether a feature is good indicator over the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define columns that we would like to test\n",
    "# col = list( X_train.columns )\n",
    "# col.remove('TransactionDT')\n",
    "# for c in ['D6','D7','D8','D9','D12','D13','D14','isFraud','Date']:\n",
    "#     col.remove(c)\n",
    "\n",
    "    \n",
    "# # SPLIT DATA INTO FIRST MONTH AND LAST MONTH\n",
    "# train = X_train[X_train.Month==12].copy()\n",
    "# validate = X_train[X_train.Month==6].copy()\n",
    "\n",
    "# # TRAIN AND VALIDATE\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# lgbm = lgb.LGBMClassifier(n_estimators=500, objective='binary', \n",
    "#         num_leaves=8, learning_rate=0.02)\n",
    "\n",
    "# #Create an empty dataframe\n",
    "# feature_auc_score = pd.DataFrame(columns = ['feature','train_score','test_score'])\n",
    "\n",
    "# #Looping through col to get train and test scores\n",
    "# for i in col:\n",
    "#     h = lgbm.fit(train[[i]], train.isFraud, eval_metric='auc', \n",
    "#             eval_set=[(train[[i]],train.isFraud),(validate[[i]],validate.isFraud)])\n",
    "#     auc_train = np.round(h._best_score['valid_0']['auc'],4)\n",
    "#     auc_val = np.round(h._best_score['valid_1']['auc'],4)\n",
    "#     feature_auc_score = feature_auc_score.append([{'feature':i,'train_score':auc_train,'test_score':auc_val}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_auc_score.to_csv('time_continuity_feature_score.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TransactionAmt</td>\n",
       "      <td>0.7301</td>\n",
       "      <td>0.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ProductCD</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>0.6187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>card1</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.6007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>card2</td>\n",
       "      <td>0.7308</td>\n",
       "      <td>0.6390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>card3</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>0.6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>D11_card1_addr1_P_emaildomain_std</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.6447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Weekdays</td>\n",
       "      <td>0.5242</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Hours</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Days</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Month</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              feature  train_score  test_score\n",
       "0                      TransactionAmt       0.7301      0.6625\n",
       "0                           ProductCD       0.6636      0.6187\n",
       "0                               card1       0.7156      0.6007\n",
       "0                               card2       0.7308      0.6390\n",
       "0                               card3       0.6468      0.6499\n",
       "..                                ...          ...         ...\n",
       "0   D11_card1_addr1_P_emaildomain_std       0.6844      0.6447\n",
       "0                            Weekdays       0.5242      0.5000\n",
       "0                               Hours       0.5682      0.5473\n",
       "0                                Days       0.5848      0.5000\n",
       "0                               Month       0.5000      0.5000\n",
       "\n",
       "[238 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionDT' 'TransactionAmt' 'ProductCD' 'card1' 'card2' 'card3'\n",
      " 'card4' 'card5' 'card6' 'addr1' 'addr2' 'dist1' 'dist2' 'P_emaildomain'\n",
      " 'R_emaildomain' 'C1' 'C2' 'C3' 'C4' 'C5' 'C6' 'C7' 'C8' 'C9' 'C10' 'C11'\n",
      " 'C12' 'C13' 'C14' 'D1' 'D2' 'D3' 'D4' 'D5' 'D6' 'D7' 'D8' 'D9' 'D10'\n",
      " 'D11' 'D12' 'D13' 'D14' 'D15' 'M1' 'M2' 'M3' 'M4' 'M5' 'M6' 'M7' 'M8'\n",
      " 'M9' 'V1' 'V3' 'V4' 'V6' 'V8' 'V11' 'V13' 'V14' 'V17' 'V20' 'V23' 'V26'\n",
      " 'V27' 'V30' 'V36' 'V37' 'V40' 'V41' 'V44' 'V47' 'V48' 'V54' 'V56' 'V59'\n",
      " 'V62' 'V65' 'V67' 'V68' 'V70' 'V76' 'V78' 'V80' 'V82' 'V86' 'V88' 'V89'\n",
      " 'V91' 'V107' 'V108' 'V111' 'V115' 'V117' 'V120' 'V121' 'V123' 'V124'\n",
      " 'V127' 'V129' 'V130' 'V136' 'V138' 'V139' 'V142' 'V147' 'V156' 'V160'\n",
      " 'V162' 'V165' 'V166' 'V169' 'V171' 'V173' 'V175' 'V176' 'V178' 'V180'\n",
      " 'V182' 'V185' 'V187' 'V188' 'V198' 'V203' 'V205' 'V207' 'V209' 'V210'\n",
      " 'V215' 'V218' 'V220' 'V221' 'V223' 'V224' 'V226' 'V228' 'V229' 'V234'\n",
      " 'V235' 'V238' 'V240' 'V250' 'V252' 'V253' 'V257' 'V258' 'V260' 'V261'\n",
      " 'V264' 'V266' 'V267' 'V271' 'V274' 'V277' 'V281' 'V283' 'V284' 'V285'\n",
      " 'V286' 'V289' 'V291' 'V294' 'V296' 'V297' 'V301' 'V303' 'V305' 'V307'\n",
      " 'V309' 'V310' 'V314' 'V320' 'id_01' 'id_02' 'id_03' 'id_04' 'id_05'\n",
      " 'id_06' 'id_07' 'id_08' 'id_09' 'id_10' 'id_11' 'id_12' 'id_13' 'id_14'\n",
      " 'id_15' 'id_16' 'id_17' 'id_18' 'id_19' 'id_20' 'id_21' 'id_22' 'id_23'\n",
      " 'id_24' 'id_25' 'id_26' 'id_27' 'id_28' 'id_29' 'id_30' 'id_31' 'id_32'\n",
      " 'id_33' 'id_34' 'id_35' 'id_36' 'id_37' 'id_38' 'DeviceType' 'DeviceInfo']\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2',\n",
       "       'card3', 'card4', 'card5', 'card6', 'addr1',\n",
       "       ...\n",
       "       'D11_card1_addr1_mean', 'D11_card1_addr1_std',\n",
       "       'D11_card1_addr1_P_emaildomain_mean',\n",
       "       'D11_card1_addr1_P_emaildomain_std', 'Date', 'Weekdays', 'Hours',\n",
       "       'Days', 'R_emaildomain_FE', 'Month'],\n",
       "      dtype='object', length=247)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list( X_train.columns )\n",
    "cols.remove('TransactionDT')\n",
    "for c in ['D6','D7','D8','D9','D12','D13','D14']:\n",
    "    cols.remove(c)\n",
    "    \n",
    "# FAILED TIME CONSISTENCY TEST\n",
    "for c in ['C3','M5','id_08','id_33']:\n",
    "    cols.remove(c)\n",
    "for c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n",
    "    cols.remove(c)\n",
    "for c in ['id_'+str(x) for x in range(22,28)]:\n",
    "    cols.remove(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
