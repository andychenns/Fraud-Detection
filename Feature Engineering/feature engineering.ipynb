{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD95 = True\n",
    "BUILD96 = True\n",
    "\n",
    "import numpy as np, pandas as pd, os, gc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# COLUMNS WITH STRINGS\n",
    "str_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n",
    "            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n",
    "            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n",
    "str_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n",
    "            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n",
    "\n",
    "# FIRST 53 COLUMNS\n",
    "cols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n",
    "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
    "       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n",
    "       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n",
    "       'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n",
    "# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\n",
    "v =  [1, 3, 4, 6, 8, 11]\n",
    "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
    "v += [36, 37, 40, 41, 44, 47, 48]\n",
    "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "\n",
    "#v += [96, 98, 99, 104] #relates to groups, no NAN \n",
    "v += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\n",
    "v += [124, 127, 129, 130, 136] # relates to groups, no NAN\n",
    "\n",
    "# LOTS OF NAN BELOW\n",
    "v += [138, 139, 142, 147, 156, 162] #b1\n",
    "v += [165, 160, 166] #b1\n",
    "v += [178, 176, 173, 182] #b2\n",
    "v += [187, 203, 205, 207, 215] #b2\n",
    "v += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\n",
    "v += [218, 223, 224, 226, 228, 229, 235] #b3\n",
    "v += [240, 258, 257, 253, 252, 260, 261] #b3\n",
    "v += [264, 266, 267, 274, 277] #b3\n",
    "v += [220, 221, 234, 238, 250, 271] #b3\n",
    "\n",
    "v += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\n",
    "v += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\n",
    "v += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n",
    "#v += [332, 325, 335, 338] # b4 lots NAN\n",
    "\n",
    "cols += ['V'+str(x) for x in v]\n",
    "dtypes = {}\n",
    "for c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n",
    "    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n",
    "        dtypes[c] = 'float32'\n",
    "for c in str_type: dtypes[c] = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (590540, 213) test shape (506691, 213)\n",
      "CPU times: user 19.7 s, sys: 734 ms, total: 20.4 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD TRAIN\n",
    "X_train = pd.read_csv('../Data/train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\n",
    "train_id = pd.read_csv('../Data/train_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
    "X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n",
    "# LOAD TEST\n",
    "X_test = pd.read_csv('../Data/test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\n",
    "test_id = pd.read_csv('../Data/test_identity.csv',index_col='TransactionID', dtype=dtypes)\n",
    "fix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\n",
    "test_id.rename(columns=fix, inplace=True)\n",
    "X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n",
    "# TARGET\n",
    "y_train = X_train['isFraud'].copy()\n",
    "del train_id, test_id, X_train['isFraud']; x = gc.collect()\n",
    "# PRINT STATUS\n",
    "print('Train shape',X_train.shape,'test shape',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE D COLUMNS\n",
    "for i in range(1,16):\n",
    "    if i in [1,2,3,5,9]: continue\n",
    "    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n",
    "    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 229 ms, total: 2.94 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LABEL ENCODE AND MEMORY REDUCE\n",
    "for i,f in enumerate(X_train.columns):\n",
    "    # FACTORIZE CATEGORICAL VARIABLES\n",
    "    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n",
    "        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n",
    "        df_comb,_ = df_comb.factorize(sort=True)\n",
    "        if df_comb.max()>32000: print(f,'needs int32')\n",
    "        X_train[f] = df_comb[:len(X_train)].astype('int16')\n",
    "        X_test[f] = df_comb[len(X_train):].astype('int16')\n",
    "    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n",
    "    elif f not in ['TransactionAmt','TransactionDT']:\n",
    "        mn = np.min((X_train[f].min(),X_test[f].min()))\n",
    "        X_train[f] -= np.float32(mn)\n",
    "        X_test[f] -= np.float32(mn)\n",
    "        X_train[f].fillna(-1,inplace=True)\n",
    "        X_test[f].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col],df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col+'_FE'\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype('float32')\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype('float32')\n",
    "        print(nm,', ',end='')\n",
    "        \n",
    "# LABEL ENCODE\n",
    "def encode_LE(col,train=X_train,test=X_test,verbose=True):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    nm = col\n",
    "    if df_comb.max()>32000: \n",
    "        train[nm] = df_comb[:len(train)].astype('int32')\n",
    "        test[nm] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[nm] = df_comb[:len(train)].astype('int16')\n",
    "        test[nm] = df_comb[len(train):].astype('int16')\n",
    "    del df_comb; x=gc.collect()\n",
    "    if verbose: print(nm,', ',end='')\n",
    "        \n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "def encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "# COMBINE FEATURES by underscore\n",
    "def encode_CB(col1,col2,df1=X_train,df2=X_test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "    \n",
    "# GROUP AGGREGATION NUNIQUE count how many times a category repeats in a column\n",
    "def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below creates new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cents, addr1_FE , card1_FE , card2_FE , card3_FE , P_emaildomain_FE , card1_addr1 , card1_addr1_P_emaildomain , card1_addr1_FE , card1_addr1_P_emaildomain_FE , 'TransactionAmt_card1_mean' , 'TransactionAmt_card1_std' , 'TransactionAmt_card1_addr1_mean' , 'TransactionAmt_card1_addr1_std' , 'TransactionAmt_card1_addr1_P_emaildomain_mean' , 'TransactionAmt_card1_addr1_P_emaildomain_std' , 'D9_card1_mean' , 'D9_card1_std' , 'D9_card1_addr1_mean' , 'D9_card1_addr1_std' , 'D9_card1_addr1_P_emaildomain_mean' , 'D9_card1_addr1_P_emaildomain_std' , 'D11_card1_mean' , 'D11_card1_std' , 'D11_card1_addr1_mean' , 'D11_card1_addr1_std' , 'D11_card1_addr1_P_emaildomain_mean' , 'D11_card1_addr1_P_emaildomain_std' , CPU times: user 13.8 s, sys: 1.71 s, total: 15.5 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRANSACTION AMT CENTS\n",
    "X_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\n",
    "X_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\n",
    "print('cents, ', end='')\n",
    "# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n",
    "encode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n",
    "# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\n",
    "encode_CB('card1','addr1')\n",
    "encode_CB('card1_addr1','P_emaildomain')\n",
    "# FREQUENCY ENOCDE for new features\n",
    "encode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n",
    "# GROUP AGGREGATE\n",
    "encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.53432918395574"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]/X_train['card1_addr1_P_emaildomain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590540"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['card1_addr1_FE'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans1 a  0.2\n",
    "trans2 a 0.2\n",
    "trans3 a 0.2\n",
    "b 0.3\n",
    "b 0.3\n",
    "c 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to feature the time of transaction because the fraud follows a pattern in a day: higher the morning and lower in the afternoon/evening. We will need to calculate the hour of a transaction for both training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "X_train[\"Date\"] = X_train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "\n",
    "X_train['Weekdays'] = X_train['Date'].dt.dayofweek\n",
    "X_train['Hours'] = X_train['Date'].dt.hour\n",
    "X_train['Days'] = X_train['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"Date\"] = X_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "\n",
    "X_test['Weekdays'] = X_test['Date'].dt.dayofweek\n",
    "X_test['Hours'] = X_test['Date'].dt.hour\n",
    "X_test['Days'] = X_test['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID\n",
       "2987000.0     0\n",
       "2987001.0     0\n",
       "2987002.0     0\n",
       "2987003.0     0\n",
       "2987004.0     0\n",
       "             ..\n",
       "3577535.0    23\n",
       "3577536.0    23\n",
       "3577537.0    23\n",
       "3577538.0    23\n",
       "3577539.0    23\n",
       "Name: Hours, Length: 590540, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Hours']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
